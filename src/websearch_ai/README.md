# Web Search Pipeline v2.0 - Modular & Configurable

> **Well-organized, production-ready web search with LLM-powered summarization**

A modular web search system with intelligent query expansion, relevance filtering, content extraction, and AI-powered summarization.

<div align="left">

[ğŸ“š <b>Documentation Hub</b> &larr;](../docs/README.md)

</div>

## ğŸ¯ Features

### Core Features
- âœ… **Clean Architecture**: Organized into core, config, clients, managers, filters
- âœ… **Modular Design**: Each component has a single, clear responsibility
- âœ… **YAML Configuration**: All parameters configurable via YAML
- âœ… **LLM-Powered**: Query improvement, relevance scoring, and summarization
- âœ… **Respectful Crawling**: Rate limiting, robots.txt compliance, domain filtering
- âœ… **File-Based Caching**: Efficient caching of fetched content
- âœ… **Type-Safe**: Pydantic models with full validation
- âœ… **Production Ready**: Comprehensive logging and error handling

### New Additions (v2.1)
- âœ… **REST API**: FastAPI-based REST API with auto-generated docs
- âœ… **Comprehensive Tests**: Unit and integration tests with pytest
- âœ… **Utility Scripts**: Tools for testing, benchmarking, and maintenance
- âœ… **Export Capabilities**: Export results to JSON, CSV, Markdown
- âœ… **Performance Benchmarks**: Measure and optimize performance
- âœ… **Configuration Validation**: Validate settings before deployment

## ğŸ“ Project Structure

```
websearch/
â”œâ”€â”€ __init__.py                 # Package exports
â”œâ”€â”€ cli.py                      # Command-line interface
â”œâ”€â”€ README.md                   # Main documentation
â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ core/                       # Core business logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py              # Data models (SearchResult, BetterQueries)
â”‚   â””â”€â”€ pipeline.py            # Main orchestration pipeline
â”‚
â”œâ”€â”€ config/                     # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py            # Settings class with validation
â”‚   â”œâ”€â”€ config.yaml            # Main configuration file
â”‚   â””â”€â”€ config.example.yaml    # Example configuration
â”‚
â”œâ”€â”€ prompts/                    # LLM prompt templates
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ prompts.yaml           # All prompt templates
â”‚
â”œâ”€â”€ clients/                    # External service clients
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ http.py                # HTTP fetcher with rate limiting
â”‚   â”œâ”€â”€ llm.py                 # OpenAI LLM client
â”‚   â””â”€â”€ search.py              # DuckDuckGo search engine
â”‚
â”œâ”€â”€ managers/                   # Core managers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cache.py               # File-based cache manager
â”‚   â”œâ”€â”€ prompts.py             # Prompt loader & formatter
â”‚   â””â”€â”€ robots.py              # Robots.txt checker
â”‚
â”œâ”€â”€ filters/                    # Content filters
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ url_filter.py          # Domain-based URL filtering
â”‚
â”œâ”€â”€ api/                        # REST API (NEW!)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                # FastAPI application
â”‚   â”œâ”€â”€ requirements.txt       # API dependencies
â”‚   â””â”€â”€ README.md              # API documentation
â”‚
â”œâ”€â”€ tests/                      # Unit tests (NEW!)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py            # Pytest fixtures
â”‚   â”œâ”€â”€ requirements.txt       # Test dependencies
â”‚   â”œâ”€â”€ test_*.py              # Test modules
â”‚   â””â”€â”€ README.md              # Testing guide
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts (NEW!)
â”‚   â”œâ”€â”€ run_tests.sh           # Run test suite
â”‚   â”œâ”€â”€ clear_cache.py         # Clear cache
â”‚   â”œâ”€â”€ validate_config.py     # Validate configuration
â”‚   â”œâ”€â”€ benchmark.py           # Performance benchmarks
â”‚   â”œâ”€â”€ export_results.py      # Export search results
â”‚   â””â”€â”€ README.md              # Scripts documentation
â”‚
â”œâ”€â”€ examples/                   # Usage examples
â”‚   â””â”€â”€ usage_examples.py      # Comprehensive examples
â”‚
â””â”€â”€ docs/                       # Documentation
    â”œâ”€â”€ README.md              # Documentation index
    â”œâ”€â”€ README_CONFIG.md       # Configuration guide
    â”œâ”€â”€ REFACTORING.md         # Refactoring details
    â”œâ”€â”€ CHANGES.md             # Changelog
    â””â”€â”€ MIGRATION_GUIDE.md     # Migration instructions
```

## ğŸš€ Quick Start

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Set your OpenAI API key
export OPENAI_API_KEY="your-api-key-here"
```

### Basic Usage

```bash
# Run a search from command line
python -m websearch.cli "latest AI developments 2025"

# Or using the module directly
cd /path/to/servers
python -m websearch.cli "your search query"
```

### Python API

```python
from websearch import Settings, WebSearchPipeline

# Load configuration from config.yaml
settings = Settings.from_yaml()

# Create and run pipeline
pipeline = WebSearchPipeline(settings)
results, final_answer = await pipeline.run("your query here")

# Access results
for result in results:
    print(f"Title: {result.title}")
    print(f"URL: {result.url}")
    print(f"Relevance: {result.relevance}/5")
    print(f"Summary: {result.summary}\n")

print(f"Final Answer:\n{final_answer}")
```

## âš™ï¸ Configuration

### YAML Configuration

Edit `config/config.yaml`:

```yaml
openai:
  model: "gpt-4.1-nano"
  temperature: 0.2

search:
  num_better_queries: 10        # Number of improved queries to generate
  max_results_per_query: 5      # Results per query

filtering:
  min_relevance_score: 3        # Minimum relevance (0-5)
  disallowed_domains:
    - youtube.com
    - youtu.be

llm_tokens:
  better_queries: 512
  relevance_check: 100
  summarize_content: 2048
  merge_summaries: 4096

fetching:
  max_concurrent_fetches: 20
  per_domain_delay: 0.8
  fetch_timeout: 30

cache:
  enabled: true
  directory: "cache_async"

logging:
  level: "INFO"
```

### Environment Variables

```bash
# Required
export OPENAI_API_KEY="sk-..."

# Optional (overrides config.yaml)
export LOG_LEVEL="DEBUG"
```

### Configuration Presets

**Fast Mode** (quick results, less thorough):
```yaml
search:
  num_better_queries: 5
  max_results_per_query: 3
filtering:
  min_relevance_score: 2
```

**Research Mode** (comprehensive, slower):
```yaml
search:
  num_better_queries: 15
  max_results_per_query: 8
filtering:
  min_relevance_score: 4
llm_tokens:
  summarize_content: 3000
  merge_summaries: 6000
```

## ğŸ—ï¸ Architecture

### Pipeline Flow

```
User Query
    â†“
1. Generate Better Queries (LLM)
    â†“
2. Perform Searches (DuckDuckGo)
    â†“
3. Filter by Relevance (LLM Scoring)
    â†“
4. Fetch Content (HTTP + Cache + Robots.txt)
    â†“
5. Summarize Each (LLM)
    â†“
6. Merge Summaries (LLM)
    â†“
Final Answer
```

### Component Responsibilities

| Component | Responsibility |
|-----------|---------------|
| **core/** | Main business logic and data models |
| **config/** | Settings management and validation |
| **prompts/** | LLM prompt templates |
| **clients/** | External service wrappers (HTTP, LLM, Search) |
| **managers/** | Cache, prompts, and robots.txt management |
| **filters/** | URL validation and filtering |
| **cli.py** | Command-line interface |
| **examples/** | Usage examples and demos |

### Key Classes

- **`WebSearchPipeline`**: Main orchestrator for the search workflow
- **`Settings`**: Configuration with Pydantic validation
- **`SearchResult`**: Type-safe search result model
- **`LLMClient`**: OpenAI API wrapper
- **`HTTPFetcher`**: Rate-limited HTTP client
- **`CacheManager`**: File-based caching
- **`URLFilter`**: Domain filtering

## ğŸ’» Usage Examples

### 1. Basic Search

```python
from websearch import Settings, WebSearchPipeline

settings = Settings.from_yaml()
pipeline = WebSearchPipeline(settings)

results, answer = await pipeline.run("quantum computing breakthroughs 2025")
print(answer)
```

### 2. Custom Configuration

```python
from websearch import Settings, WebSearchPipeline

settings = Settings(
    openai_api_key="your-key",
    search_num_better_queries=15,
    min_relevance_score=4,
    log_level="DEBUG"
)

pipeline = WebSearchPipeline(settings)
results, answer = await pipeline.run("climate change solutions")
```

### 3. Using Individual Components

```python
from websearch.clients import SearchEngine
from websearch.config import Settings

settings = Settings.from_yaml()
search_engine = SearchEngine(settings)

results = search_engine.search("machine learning", max_results=10)
for r in results:
    print(f"{r['title']}: {r['url']}")
```

### 4. Custom Domain Filtering

```python
from websearch import Settings, WebSearchPipeline

settings = Settings.from_yaml()
settings.disallowed_domains = [
    "youtube.com", "twitter.com", "facebook.com"
]

pipeline = WebSearchPipeline(settings)
results, answer = await pipeline.run("social media trends")
```

## ğŸ§ª Testing

```bash
# Test imports
python -c "from websearch import *; print('âœ… All imports OK')"

# Run examples
cd /path/to/servers/websearch
python examples/usage_examples.py

# Run CLI with test query
python -m websearch.cli "test query"
```

## ğŸ“Š Statistics

- **Total Files**: 35+ Python files + YAML configs
- **Code Modules**: 16 core files
- **Test Files**: 7 test modules
- **Utility Scripts**: 5 scripts
- **API Endpoints**: 5 REST endpoints
- **Average File Size**: ~65 lines
- **Longest File**: pipeline.py (~289 lines)
- **Test Coverage**: Unit + Integration tests
- **Lint Status**: âœ… Clean
- **API Documentation**: Auto-generated (Swagger + ReDoc)

## ğŸ†• What's New in v2.1

### REST API
Full-featured REST API with FastAPI:
```bash
# Start API server
python -m uvicorn websearch.api.main:app --reload

# Visit interactive docs
open http://localhost:8000/docs
```

### Comprehensive Tests
Unit and integration tests with pytest:
```bash
# Run all tests
pytest tests/

# With coverage report
pytest tests/ --cov=websearch
```

### Utility Scripts
Production-ready tools:
```bash
# Validate configuration
python scripts/validate_config.py

# Benchmark performance
python scripts/benchmark.py

# Export results to JSON/CSV/Markdown
python scripts/export_results.py "query" --format json

# Clear cache
python scripts/clear_cache.py
```

See the [API README](api/README.md), [Tests README](tests/README.md), and [Scripts README](scripts/README.md) for details.

## ğŸ”„ Migration from Old Structure

### Import Changes

**Before:**
```python
from web_search_improved import Settings, WebSearchPipeline
from config import Settings
from models import SearchResult
```

**After:**
```python
from websearch import Settings, WebSearchPipeline
from websearch.config import Settings
from websearch.core import SearchResult
```

### File Locations

| Old | New |
|-----|-----|
| `config.py` | `config/settings.py` |
| `models.py` | `core/models.py` |
| `pipeline.py` | `core/pipeline.py` |
| `web_search_refactored.py` | `cli.py` |
| `example_usage.py` | `examples/usage_examples.py` |
| `config/prompts.yaml` | `prompts/prompts.yaml` |

## ğŸ› ï¸ Development

### Adding New Features

**New Search Engine:**
```python
# clients/brave_search.py
from ..config import Settings

class BraveSearchEngine:
    def __init__(self, settings: Settings):
        self.settings = settings
    
    def search(self, query: str, max_results: int):
        # Implementation
        pass
```

**New Filter:**
```python
# filters/content_filter.py
class ContentFilter:
    def filter(self, content: str) -> bool:
        # Implementation
        pass
```

**New Cache Backend:**
```python
# managers/redis_cache.py
class RedisCache:
    async def read(self, key: str):
        pass
    
    async def write(self, key: str, value: str):
        pass
```

### Code Style

- Use type hints for all function parameters and returns
- Add docstrings to all classes and public methods
- Keep files focused and under 300 lines
- Use relative imports within the package
- Follow PEP 8 naming conventions

## ğŸ“ Dependencies

- `openai` - LLM API client
- `pydantic` - Data validation
- `aiohttp` - Async HTTP client
- `trafilatura` - Content extraction
- `ddgs` - DuckDuckGo search
- `pyyaml` - YAML configuration
- `aiofiles` - Async file operations

## ğŸ¤ Contributing

The modular structure makes contributions easy:
1. Each component is independent with clear interfaces
2. Easy to test individual components
3. Minimal cross-component dependencies
4. Well-documented code

## ğŸ“„ License

[Your License Here]

## ğŸ™ Acknowledgments

- OpenAI for GPT models
- DuckDuckGo for free search API
- Trafilatura for content extraction

---

**Version**: 2.0.0  
**Status**: âœ… Production Ready  
**Last Updated**: November 2025

For detailed documentation, see the `docs/` directory.

